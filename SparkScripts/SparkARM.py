'''

Association Rule Mining using Frequency Pattern Mining. 
GOAL OF ALGORITHM: 
The goal of this algorithm is to assign a similarity score to items based on the frequency of them appearing in similar item lists.
This algorithm uses frequency mining to find the relevant frequencies of items appearing on similar item lists. This then uses association rules to find the pattern of copurchasing. 
The association rules look at the likelihood that people by item Y (consequent) after buying X (antecedent). It generates these rules based on a confidence rating which is generated by the formula (Support(X U Y) / Support(X)). 
This compares the number of times X and Y are found together by the total number of times X is found. The value of support shows the frequency of the rule in the dataset. The final measure is lift which is the strength of a rule. 
Lift shows the likelihood that the two items are bought together. The larger the number for the lift, the stronger likelihood that they will be bought together rather than separately. 

BEFORE YOU RUN:
This algorithm will generate and fill two collections in the mongo database specified by the spark configuration. Make sure that the spark configuration is set to the proper MongoDB before running. 
In my testing my MONGO had three databases:
1. ProductMetaData: 534k Documents
2. FrequentItemSets: 457k Documents
3. Association Rules: 799k Documents
(Association Rules contains more documents than both FrequentItemSets and ProductMetaData because multiple items can exist under one item's similarItems list.)

'''
import time
from pyspark import SparkConf
from pyspark.sql import SparkSession
from pyspark.ml.fpm import FPGrowth
from pyspark.sql.functions import col
from pyspark.sql import functions as F
from pyspark.sql.functions import explode

# 1. Launch Spark
start_time_total = time.time()
#Configure Spark to connect to mongo and adjust memory
conf = SparkConf() \
    .setAppName("CoPurchasingAnalysis") \
    .set("spark.mongodb.input.uri", "mongodb://localhost:27017/AmazonProject.ProductMetaData") \
    .set("spark.mongodb.output.uri", "mongodb://localhost:27017/AmazonProject") \
    .set("spark.executor.memory", "4g") .set("spark.driver.memory", "4g") .set("spark.memory.fraction", "0.8") # Set executor and driver memory to 4g in order to produce a larger frequency list
                                                                                                               # This can be reduced when minSupport is reduced but will lead to a smaller frequency list as a result.
#Build and launch Spark
spark = SparkSession.builder \
    .config(conf=conf) \
    .config("spark.jars.packages", "org.mongodb.spark:mongo-spark-connector_2.12:3.0.1") \
    .getOrCreate()
spark.sparkContext.setLogLevel("ERROR")
# 2. Load Data
print("Loading data from MongoDB...")
df = spark.read.format("mongo").load()
df.show(5)  # Display first 5 rows
df.printSchema()  # Display schema

# 3. Filter and Extract
print("Filtering and extracting relevant columns...")
start_time_filter = time.time()
filtered_df = df.filter(F.col("similar.items").isNotNull()) \
                .filter(F.size(col("similar.items")) >= 1)  # Keep only rows where `items` has at least one item

# Remove the number of items from each array 
cleaned_df = filtered_df.withColumn("items", F.array_except(col("similar.items"), F.array(F.lit("5"), F.lit("0"), F.lit("1"), F.lit("2"), F.lit("3"), F.lit("4"))))

# Remove empty arrays
cleaned_df = cleaned_df.filter(F.size(col("items")) > 0)

# Display first 5 rows 
cleaned_df.select("ASIN", "items").show(5, truncate=False)

# Explode arrays so each row has one item per ASIN
exploded_df = cleaned_df.withColumn("item", explode(col("items")))

# Group the exploded rows back into a list of items for each ASIN
transactions_df = exploded_df.groupBy("ASIN").agg(F.collect_list("item").alias("items"))
transactions_df.show(5, truncate=False)
end_time_filter = time.time()
print(f"Data cleaning took {end_time_filter - start_time_filter} seconds")

# 4. Use FP-Growth for frequency mining
print("Applying FP-Growth for frequency mining...")
start_time = time.time()
fpgrowth = FPGrowth(itemsCol="items", minSupport=0.00001, minConfidence=0.001)
model = fpgrowth.fit(transactions_df)
end_time = time.time()
print(f"FP-Growth fitting took {end_time - start_time} seconds")

# 5. Retrieve Frequent Items and Association Rules
start_time_fetch = time.time()
print("Fetching frequent itemsets...")
frequent_itemsets = model.freqItemsets
frequent_itemsets.show(10, truncate=False)

print("Fetching association rules...")
association_rules = model.associationRules
association_rules.show(10, truncate=False)
end_time_fetch = time.time()
print(f"Fetching frequentItemSets and AssociationRules took {end_time_fetch - start_time_fetch} seconds")
# 6. Save output to MongoDB
if frequent_itemsets.count() == 0:
    print("No frequent itemsets found.")
else:
    print(f"Frequent Itemsets: {frequent_itemsets.count()} found.")
    print("Saving frequent itemsets to MongoDB...")
    frequent_itemsets.write.format("mongo") \
        .option("uri", "mongodb://localhost:27017/AmazonProject.FrequentItemsets") \
        .mode("overwrite") \
        .save()

if association_rules.count() == 0:
    print("No association rules found.")
else:
    print(f"Association Rules: {association_rules.count()} found.")
    print("Saving association rules to MongoDB...")
    association_rules.write.format("mongo") \
        .option("uri", "mongodb://localhost:27017/AmazonProject.AssociationRules") \
        .mode("overwrite") \
        .save()
end_time_total = time.time()
print(f"Total script took {end_time_total - start_time_total} seconds")
print("Associative rule mining process completed.")
